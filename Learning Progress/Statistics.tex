% Created 2018-05-30 Wed 11:07
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{yeldon}
\date{\textit{[2018-05-29 Tue 15:09]}}
\title{Zhouteng Ye's learning progress on probability and statistics}
\hypersetup{
 pdfauthor={yeldon},
 pdftitle={Zhouteng Ye's learning progress on probability and statistics},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.2 (Org mode 9.1.13)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{History of probability and statistics}
\label{sec:orgd1f6257}
\textbf{Probability} has been defined as the study of the frequency of appearance of a
phenomenon in relation to all possible alternatives. 

\textbf{Statistics} is the science and art of gathering, analyzing, and making
inferences from data.


The history of probability and statistics:
\begin{itemize}
\item Randomness was not associated with gaming in early time. Starting from
Renaissance, randomness started being considered.
\item "Problem of points" gives rise to the science of mathematical probability.
Discussion among Paciolo, Pascal, Fermat and Mere not only came up with a
convincing, self-consistent solution to the division of the stakes, but also
developed concepts that continue to be fundamental in probability to this day.
\item In 1657, Huygens wrote the first formal treatise on probability \emph{On reasoning
in games of chance} based on the Fermat-Pascal correspondence. In 1713 Jakob
Bernouli pushed the book \emph{The art of conjecturing}. Bernoulli also proved a
version of the fundamental law of large numbers.
\item John Graunt, "Father of Statistics", was the first person drew statistical
inferences from analyses of mass data in 17th century.
\item Halley, Newton, Demoivre made pioneering efforts on actuarial mathematics and
its relation to life insurance. During which the mathematics of permutation,
combination and normal probability curve has been developed.
\item After that, probability and statistics entered a new transitional period.
mathematicians realized that probability could not be separated from
statistics.
\item Daniel Bernoulli proposed Petersburg paradox (However, the name took from the
resolution of Daniel Bernoulli), calculus was applied to the theory of
probability. Euler, Lagrange also advanced the theory of probability by
applying differential calculus to it.
\item Laplace, "Father of Modern Probability Theory" used the theory of probability
to obtain a statistical measure of reliability of numerical results. The
results relies on definite causes rather than pure chance.
\item Using the probability theory that various types of mathematical distributions
has been proposed (Bernoulli, Poisson and normal distribution).
\item The beginning of statistical analysis of census data was accomplished in 1829
by Lambert Adolphe Jacques Queteletm.
\item Father Gregor Mendel related probability to genetics and hybridization
in 1865.
\item Galton discovered the law of regression and the correlation coefficient
in 1877.
\item Beginning in 1894, Karl Pearson applied probability to biology and created the
area of study we now call biometric.
\item The Russian Andrey Markov developed his chain theory of probabilities
\item Norbert Wiener expressing his belief that probability is the link between
physics and mathematics, created cybernetics.
\end{itemize}


\section{Study of evidence theory}
\label{sec:orgbf8120a}

\subsection{Definition of evidence theory}
\label{sec:org8d0a8e9}

Evidence theory (Dempster-Shafer theory) is a general framework for reasoning
with uncertainty, with understood connections to other frameworks such as
probability, possibility and imprecise probability theories.

\begin{itemize}
\item Heart of the evidence theory: \textbf{Dempster's rule}.
\item The theory deals with weights of evidence and with numerical degree of support
based on evidence.
\item Does not focus on the act of judgment by which such a number is determined.
\item Focus on something more amenable to mathematical analysis: the combination on
something degrees of belief or support based on on body evidence with those
based on an entirely distinct body of evidence.
\end{itemize}

A \textbf{degree of belief} is represented as a \textbf{belief function}. Probability values
are assigned to sets of possibilities rather than single events.



\subsection{Belief Function}
\label{sec:org47e734b}

    Suppose \(\Theta\) is a finite set, and let \(2 ^ \Theta\) denote the set of all
subsets of \(\Theta\), Suppose the function \emph{Bel}: \(2^\Theta \rightarrow [0,1]\)
satisfies the following conditions:
\begin{itemize}
\item (1) \(Bel (\Phi)=0\)
\item (2) \(Bel (\Theta) = 1\)
\item (3) For every positive integer \(n\) and every collection \(A_1,..,A_n\) of
subsets of \(\Theta\),
\end{itemize}
\(Bel(A_1\cup ... \cup A_n) \ge \sum_i Bel(A_i) - \sum_{i<j} Bel(A_i \cap A_j)
+-...+(-1)^{n+1}Bel(A_1\cap...\cap A_n)\)

Then \emph{Bel} is called a \textbf{belief function} over \(\Theta\).

    Only the set functions obey the rules (1) to (3) can be combined by \textbf{Dempster's
rule of combination}.

\subsection{The idea of chance}
\label{sec:org49d202e}

    The ideas of \textbf{Chance} and \textbf{belief} are united under the name \textbf{probability}. But
they actually have different roles to play. \textbf{Chance} arises only when one
describes an aleatory (or random) experiment. The outcome varies randomly from
one physically independent trial to another.

\textbf{Basic rules for chances}: A function \(Ch\): \(2^X \rightarrow [0,1]\) is a chance
function if and only if it obeys the following rules:
\begin{itemize}
\item (1) \(Ch(\Phi) = 0\).
\item (2) \(Ch(X) = 1\).
\item (3) If \(U, V \subset X\) and \(U\cap V = \Phi\), then \(Ch(U\cup V) = Ch(U) +
  Ch(V)\).
\end{itemize}
The Third rule is *Rule of additivity for chances.

\textbf{\emph{Chance can be used to estimate the degrees of belief. But it is merely used as
the chance density governing an experiment is usually unknown.}}

\subsection{Bayesian theory of partial belief}
\label{sec:org67d756d}
the function \emph{Bel}: \(2^\Theta \rightarrow [0,1]\) in Bayesian theory must satisfies the following conditions:

\begin{itemize}
\item (1) \(Bel (\Phi)=0\)
\item (2) \(Bel (\Theta) = 1\)
\item (3) If \(A\cap B = \Phi\), then \(Bel(A\cup B) = Bel(A) + Bel(B)\)
\item (4) If \(Bel(A)>0, then Bel(B|A) =\frac{Bel(B\cap A)}{Bel(A)}\).
\end{itemize}

Rule (3) is called \textbf{Bayes' rule of additivity}, rule (4) is called \textbf{Bayes' rule
of conditioning}.

\subsection{Evidence theory and Bayesian theory}
\label{sec:org05d5cdc}

The first two rules from Bayesian theory are identical to the first two rules for
belief function. The third rule is different between two theories. All function
satisfying Bayesian belief function satisfies the third rule in evidence theory;
but not all belief function satisfies Bayes' rule of additivity. \textbf{\emph{In other words, Bayesian theory is a restrictive spacial case in the theory of evidence; Theory of evidence is a generalization of Bayesian theory.}}



\subsection{Support function and weight of evidence}
\label{sec:org5255cfe}

A belief function \(Bel:2^\Theta \rightarrow [0,1]\) is called \textbf{Simple support
function} if there exists a non-empty subset \(A\) of \(\Theta\) and a number \(s\),
\(0\le s \le 1\), such that
\begin{itemize}
\item \(Bel(B) = 0\), if \(B\) does not contain \(A\)
\item \(Bel(B) = s\), if \(B\) contains \(A\), but \(B \ne \Theta\)
\item \(Bel(B) = 1\), if \(B = \Theta\)
\end{itemize}

Combination of simple support functions supporting same subset \(A\) leads to a
larger class of belief function, called \textbf{Separable support function}. 

The \textbf{Support function} is the function that includes all belief functions that
can be obtained by beginning with a separate support function on a certain set
of possibilities and then "coarsen" the set of possibility by neglecting to
distinguish between certain of its elements.Bayes' belief function should be qualified as \textbf{Quasi support function}, which is
obtained by the limit of a sequence of support function.

The idea of a \textbf{Weight of evidence} \(w\) is closely related to support function. When
\(s\) is the degree of support, the relationship between \(s\) can be described as

\(s = 1-e^{-w}\).

\subsection{Steps of evidence theory approach}
\label{sec:orgf862c7f}

The ET approach for quantifying the uncertainty in the performance of a system
and assessing the safety of the system consists of three steps:
\begin{itemize}
\item (a) Model uncertainty. First, models considering each variable separately are
constructed. Then a model that considers all variables together is derived.
\item Propagate uncertainty through the system. This step results in a model of the
uncertainty in the performance of the system.
\item Assess the system safety.
\end{itemize}


\subsection{Recent development of evidence theory}
\label{sec:orgbe759a6}
The classical form of Dempster combination rule in Dempster-Shafer theory is
incapable dealing with high conflicting evidence. The modified theory of
evidence pursuits to handle conflicting evidence. Recent works, for example,
Deng (2015) proposed generalized evidence theory (GET) theory with the consideration of two
main causes of evidence conflict: 
\begin{itemize}
\item Sensor reliability caused by disturbances or the condition of equipment.
\item Our knowledge is not complete.
\end{itemize}
Jiang and Zhan (2017) found that generalized combination rules (GCR) proposed by
Deng (2015) could also produce unreasonable results when combining generalized
basic probability assignment. They modified the GCR is proposed to provide an
aspect from geometry to explain the combination rule.


\subsection{Evidence theory in machine learning}
\label{sec:orga84257b}
I have not got enough time to read about it, but I do see some paper on this topic, mostly using evidence theory for classifiers.

\section{Reading lists}
\label{sec:orgd3ab461}
\begin{itemize}
\item \href{http://www.jstor.org/stable/pdf/27967334.pdf?refreqid=excelsior\%253A8eea78ccaa3fd539bb77e2b2345460d7}{Lightner, J.E., 1991. A brief look at the history of probability and
statistics. The Mathematics Teacher, 84(8), pp.623-630.}
\item \href{http://dspace.elib.ntt.edu.vn/dspace/bitstream/123456789/8106/1/Introduction\%2520to\%2520Probability\%2520Models.pdf}{Ross, S.M., 2014. Introduction to probability models. Academic press.}
\item \href{https://libgen.pw/download/book/5a1f052a3a044650f50c7b6b}{Shafer, G., 1976. A mathematical theory of evidence (Vol. 42). Princeton
university press.}
\item \href{https://web.stanford.edu/group/cits/pdf/lectures/oberkampf.pdf}{Oberkampf, W.L., 2005, August. Uncertainty quantification using evidence
theory. In Proceedings from the Advanced Simulation \& Computing Workshop.}
\item \href{https://www.sciencedirect.com/science/article/pii/S0951832004000675}{Soundappan, P., Nikolaidis, E., Haftka, R.T., Grandhi, R. and Canfield,
R., 2004. Comparison of evidence theory and Bayesian theory for uncertaint
modeling. Reliability engineering \& System safety, 85(1-3), pp.295-311.}
\item \href{https://link.springer.com/article/10.1007/s10489-015-0661-2}{Deng, Y., 2015. Generalized evidence theory. Applied Intelligence, 43(3),
pp.530-543.}
\item \href{https://link.springer.com/article/10.1007/s10489-016-0851-6}{Jiang, W. and Zhan, J., 2017. A modified combination rule in generalized
evidence theory. Applied Intelligence, 46(3), pp.630-640.}
\item \href{https://en.wikipedia.org/wiki/Problem\_of\_points}{Problem of points on wikipedia}
\item \href{https://en.wikipedia.org/wiki/History\_of\_probability}{History of probability on wikipedia}
\item \href{https://en.wikipedia.org/wiki/History\_of\_statistics}{History of statistics on wikipedia}
\item \href{https://en.wikipedia.org/wiki/Dempster\%25E2\%2580\%2593Shafer\_theory}{Dempster-Shafer theory on wikipedia}
\item \href{https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/}{Statistics for Applications on MIT OpenCourWare}
\end{itemize}
\end{document}
